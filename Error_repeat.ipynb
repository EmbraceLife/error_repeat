{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:31:24.133174Z",
     "start_time": "2017-03-07T00:30:31.192546+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    books/pride_and_prejudice.txt\n",
      "A    books/shakespeare.txt\n",
      "Checked out revision 371.\n"
     ]
    }
   ],
   "source": [
    "!svn checkout https://github.com/deepgram/kur/trunk/examples/language-model/books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:31:30.037273Z",
     "start_time": "2017-03-07T00:31:30.020917+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting make_data_func.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_data_func.py\n",
    "# vocab.py\n",
    "\n",
    "# length of the context sequence                        # what does context sequence mean? \n",
    "                                                        # a sequence of words used to predict the next word\n",
    "# seq_len = 30\n",
    "\n",
    "                                                        # to create a list of 26 alphabet letter\n",
    "lowercase_letters = [                             \n",
    "    chr(97 + i) for i in range(26)\n",
    "]\n",
    "\n",
    "                                                        # define a list of 4 symbols: space, \", \\, .\n",
    "symbols = [' ', '\"', '\\'', '.']             ############# QUESTION: all other numbers and symbols are ignored, ### \n",
    "                                                        # in what situations this is common practice on vocab? ###\n",
    "                                                        # in what situations should we keep all num and symbols ##\n",
    "\n",
    "                                                        # define a list of unique characters we allow in our data\n",
    "vocab = lowercase_letters + symbols\n",
    "\n",
    "                                                        # create a dictionary: {char:index}\n",
    "                                                        # give each character an index\n",
    "char_to_int = dict(\n",
    "    (c, i) for i, c in enumerate(vocab)\n",
    ")\n",
    "                                                        # create a dictionary: {index:char}\n",
    "                                                        # give each index an character\n",
    "int_to_char = dict(enumerate(vocab))\n",
    "                                                        # get the length of vocab\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# make_data.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# from vocab import *\n",
    "import json\n",
    "import os\n",
    "\n",
    "                                                 # given 30 character in sequence to predict the next character\n",
    "                                                 # dev = True, is to only use 10% of data\n",
    "def make_data(seq_len=30, dev=True):\n",
    "\n",
    "    if not os.path.exists('./data/'):             ### create a data folder if not already available ###\n",
    "        os.mkdir('./data/')\n",
    "\n",
    "                                                  ### convert character from indices to one-hot-encoding ###\n",
    "    def one_hot(v, ndim):                         # v: indices of all the unique characters of a text\n",
    "        v_one_hot = np.zeros(                     # ndim: num of all unique characters of the text\n",
    "            (len(v), ndim,)\n",
    "        )\n",
    "        for i in range(len(v)):\n",
    "            v_one_hot[i][v[i]] = 1.0              # for each unique character, make 1 at its index on column\n",
    "        return v_one_hot\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    all_chars = []\n",
    "\n",
    "                                                  # for each book\n",
    "    for book in [\n",
    "        'pride_and_prejudice.txt',\n",
    "        'shakespeare.txt'\n",
    "    ]:                                          \n",
    "                                                  # for every line of this book\n",
    "        with open('books/%s' % book, 'r') as infile:\n",
    "                                                  # split every word into separate characters, a space separate each word\n",
    "            chars = [\n",
    "                c for c in ' '.join(infile.read().lower().split())\n",
    "                if c in set(vocab)\n",
    "            ]\n",
    "            all_chars += [' ']\n",
    "            all_chars += chars\n",
    "                                                  # put all words of book into a long list (see **example1** below)\n",
    "\n",
    "\n",
    "                                                  # get rid of the space in the beginning\n",
    "    all_chars = list(' '.join(''.join(all_chars).split()))\n",
    "    num_chars = len(all_chars)                    # count num of characters in the book including spaces\n",
    "\n",
    "\n",
    "                                                  # create an empty file named `cleaned.txt` to write data into\n",
    "    with open('cleaned.txt', 'w') as outfile:\n",
    "                                                  # write all the whole list of characters of the books into it\n",
    "        outfile.write(''.join(all_chars))         # without a space in the beginning\n",
    "\n",
    "\n",
    "    x, y = [], []                                 # set x, y as empty list\n",
    "\n",
    "\n",
    "                                                  # define portions for each section: train, validate, evalute, test\n",
    "    data_portions = [\n",
    "        ('train', 0.8),\n",
    "        ('validate', 0.05),\n",
    "        ('test', 0.05),\n",
    "        ('evaluate', 0.05),\n",
    "    ]\n",
    "\n",
    "#     dev = True                                     # reduce amount of data for training x10 times\n",
    "    if dev:\n",
    "                                                   # shrink every section data by x10 times\n",
    "\n",
    "        for i in range(len(data_portions)):\n",
    "            data_portions[i] = (\n",
    "                data_portions[i][0],\n",
    "                data_portions[i][1] * 0.1\n",
    "            )\n",
    "\n",
    "\n",
    "                                                   # sum up num of data points in each section (train, validate...) \n",
    "                                                   # max_i = sum_above - seq_len\n",
    "    max_i = sum([\n",
    "        int(round(len(all_chars) * fraction))\n",
    "        for name, fraction in data_portions\n",
    "    ]) - seq_len                                   # seq_len is defined inside vocab.py as 30\n",
    "\n",
    "\n",
    "                                                   # for every element of max_i \n",
    "    for i in range(max_i):\n",
    "                                                   # create a short list of length 30, assign to in_char_seq\n",
    "                                                   # every in_char_seq only has 1 character different to its neighbour\n",
    "        in_char_seq = all_chars[i: i + seq_len]\n",
    "\n",
    "                                                   # one hot representation\n",
    "                                                   # create a matrix of 0s with dim(30, 30), assigned to sample_x\n",
    "        sample_x = np.zeros((len(in_char_seq), n_vocab,))\n",
    "\n",
    "\n",
    "        for j, c in enumerate(in_char_seq):        # j as index from 0 to 29, c as character \n",
    "            sample_x[j][char_to_int[c]] = 1        # find unique index of the character, and put 1 in the index of column\n",
    "                                                   # by now, a list of 30 characters turned into a matrix of 0s and 1s \n",
    "                                                   # in other words, 30 character in one-hot-encoding format \n",
    "\n",
    "        x.append(sample_x)                         # tranform all characters into one-hot-encoding format and save them\n",
    "                                                   # all in x (x was an empty list)\n",
    "                                                   # what does x look like? - a list of arrays see **example2** #######\n",
    "\n",
    "\n",
    "                                                   # create a 1-d array of 0s of length 30 for sample_y \n",
    "        sample_y = np.zeros(n_vocab)\n",
    "\n",
    "                                                   # get all characters from the 30th onward\n",
    "                                                   # get the unique index of the character\n",
    "                                                   # make the location of the index from 0 to 1\n",
    "        sample_y[char_to_int[all_chars[i + seq_len]]] = 1\n",
    "        y.append(sample_y)                         # now, we transformed the y from character to one-hot-encoding\n",
    "\n",
    "\n",
    "                                                   # convert x, y from lists to arrays of arrays\n",
    "    x, y = np.array(x).astype('int32'), np.array(y).astype('int32') \n",
    "                                                   # x as 3-d (max_i, 30, 30), y as 2-d (max_i, 30)  \n",
    "                                                                                            # see **example2** #######\n",
    "\n",
    "\n",
    "                                                    # set starting index\n",
    "    start_i = 0     \n",
    "                                                    # for each section: train, validate, evaluate, test\n",
    "    for name, fraction in data_portions:\n",
    "                                                    # get ending index of each section\n",
    "        end_i = start_i + int(round(len(x) * fraction))    # len(x) == max_i\n",
    "        print(start_i, end_i)\n",
    "        x0 = x[start_i: end_i]\n",
    "        y0 = y[start_i: end_i]\n",
    "                                                    # print dim of each section's x and y\n",
    "        print('dims:')\n",
    "        print(x0.shape)\n",
    "        print(y0.shape)\n",
    "                                                    # set current section's ending index as starting index of next section\n",
    "        start_i = end_i\n",
    "                                                    # open an empty jsonl file and write to it\n",
    "        with open('data/%s.jsonl' % name, 'w') as outfile:\n",
    "            for sample_x, sample_y in zip(x0, y0):  # zip sample_x and sample_y together\n",
    "                outfile.write(json.dumps({\n",
    "                    'in_seq': sample_x.tolist(),    # write into file as list of list rather than numpy arrays\n",
    "                    'out_char': sample_y.tolist()   # QUESTION: it means kur accept lists from jsonl file by default\n",
    "                }))\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        del x0, y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:31:36.122467Z",
     "start_time": "2017-03-07T00:31:31.644301+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13302\n",
      "dims:\n",
      "(13302, 2, 30)\n",
      "(13302, 30)\n",
      "13302 14133\n",
      "dims:\n",
      "(831, 2, 30)\n",
      "(831, 30)\n",
      "14133 14964\n",
      "dims:\n",
      "(831, 2, 30)\n",
      "(831, 30)\n",
      "14964 15795\n",
      "dims:\n",
      "(831, 2, 30)\n",
      "(831, 30)\n"
     ]
    }
   ],
   "source": [
    "from make_data_func import *\n",
    "                                        # we only use a sequence of 2 characters to predict\n",
    "data = make_data(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:31:37.373419Z",
     "start_time": "2017-03-07T00:31:37.365319+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rnn_demo_dlnd_defaults.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rnn_demo_dlnd_defaults.yaml\n",
    "\n",
    "---\n",
    "\n",
    "settings:\n",
    "\n",
    "\n",
    "  vocab:                                         \n",
    "    size: 30                          # This cannot be changed, it is fixed with dataset           \n",
    "                \n",
    "  \n",
    "\n",
    "# QUESTION: what cause the following error, given the model build fine. see `build` in the next cell               \n",
    "# we get the following error meassage: \n",
    "# Traceback (most recent call last):\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 224, in train\n",
    "#     **kwargs\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 648, in wrapped_train\n",
    "#     raise ValueError('Model loss is NaN.')\n",
    "# ValueError: Model loss is NaN.\n",
    "# \n",
    "# During handling of the above exception, another exception occurred:\n",
    "# \n",
    "# Traceback (most recent call last):\n",
    "#   File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/bin/kur\", line 11, in <module>\n",
    "#     load_entry_point('kur', 'console_scripts', 'kur')()\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 382, in main\n",
    "#     sys.exit(args.func(args) or 0)\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 62, in train\n",
    "#     func(step=args.step)\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 371, in func\n",
    "#     return trainer.train(**defaults)\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 246, in train\n",
    "#     info={'Reason' : reason}\n",
    "#   File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/hooks/plot_hook.py\", line 123, in notify\n",
    "#     vbatch = numpy.arange(1, len(vloss)+1)\n",
    "# TypeError: object of type 'NoneType' has no len()\n",
    "# CPU times: user 511 ms, sys: 504 ms, total: 1.02 s\n",
    "# Wall time: 31.6 s\"\"\"    \n",
    "\n",
    "# ANSWER: try remove batch_normalization or change to gru, or change optimizers\n",
    "# it turns out change from lstm to gru works \n",
    "\n",
    "## Another option is to use `lstm` with `hard_sigmoid`, but not workign here \n",
    "# see https://github.com/deepgram/kur/issues/7#issuecomment-282916382\n",
    "\n",
    "  rnn:\n",
    "    size: 512                                    # num_neurons of a rnn/lstm layer\n",
    "    depth: 2                                     # num_rnn_layers for this RNN model\n",
    "        \n",
    "\n",
    "\n",
    "model:\n",
    "  - input: in_seq\n",
    "\n",
    "\n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: lstm\n",
    "      sequence: True                             # yes, meaning return the whole sequence of 30 characters??\n",
    "      bidirectional: no\n",
    "  - batch_normalization\n",
    "  - dropout: \"{{drop_neurons}}\"\n",
    "        \n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: lstm                           \n",
    "      sequence: False                     # no, meaning only return only 1 character of the whole 30 char sequence??\n",
    "      bidirectional: no\n",
    "  - batch_normalization\n",
    "  - dropout: \"{{drop_neurons}}\"\n",
    "\n",
    "\n",
    "  - dense: \"{{ vocab.size }}\"                   # now it is like 30 class-classification problem, \n",
    "                                                # that's why we need 30 neurons here, right? \n",
    "\n",
    "  - activation: softmax\n",
    "\n",
    "  - output: out_char                               # make a name of output layer\n",
    "           \n",
    "\n",
    "loss:\n",
    "  - target: out_char\n",
    "    name: categorical_crossentropy\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - jsonl: data/train.jsonl\n",
    "  epochs: \"{{ num_epochs|default(5) }}\"     \n",
    "  weights:\n",
    "    initial: t3_dlnd/best.w.kur\n",
    "    best: t3_dlnd/best.w.kur\n",
    "    last: t3_dlnd/last.w.kur\n",
    "  log: t3_dlnd/log\n",
    "  optimizer: \n",
    "    name: adam\n",
    "  hooks:                                   \n",
    "    - plot: t3_dlnd/loss.png\n",
    "        \n",
    "                                # QUESTION: by default optimizer is Adam, lr = 0.001\n",
    "                                # given 'grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)`\n",
    "                                # I should set clip as follows, right?\n",
    "  clip:                                              \n",
    "    norm: \"{{grad_clip}}\"                            \n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - jsonl: data/validate.jsonl\n",
    "  weights: t3_dlnd/best.w.kur\n",
    "\n",
    "\n",
    "test:\n",
    "  data:\n",
    "    - jsonl: data/test.jsonl\n",
    "  weights: t3_dlnd/best.w.kur\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  data:\n",
    "    - jsonl: data/evaluate.jsonl\n",
    "  weights: t3_dlnd/best.w.kur\n",
    "\n",
    "  destination: t3_dlnd/output.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:31:39.077417Z",
     "start_time": "2017-03-07T00:31:39.070950+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rrn_demo_dlnd_fluid.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rrn_demo_dlnd_fluid.yaml\n",
    "\n",
    "---\n",
    "settings: \n",
    "  num_epochs: 1                    # leave it empty means inf number of epochs\n",
    "                                 # so to use default value, just comment this line out\n",
    "  drop_neurons: 0.5\n",
    "  grad_clip: 5\n",
    "\n",
    "include: char_rnn_demo_dlnd_defaults.yaml\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T16:32:09.703580Z",
     "start_time": "2017-03-07T00:31:41.277841+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-07 00:31:42,429 kur.kurfile:699]\u001b[0m Parsing source: char_rrn_demo_dlnd_fluid.yaml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:42,434 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_demo_dlnd_defaults.yaml, included by char_rrn_demo_dlnd_fluid.yaml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:42,451 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:42,472 kur.loggers.binary_logger:71]\u001b[0m Loading log data: t3_dlnd/log\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:43,012 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:43,012 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:43,012 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:44,075 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:44,075 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:44,075 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:44,076 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:46,443 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:46,443 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:46,444 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: t3_dlnd/best.w.kur. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:46,444 kur.model.executor:315]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:46,444 kur.model.executor:323]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:31:46,444 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:32:07,198 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\n",
      "Epoch 1/1, loss=5.340:   0%|           | 32/13302 [00:00<00:55, 237.34samples/s]\u001b[1;31m[ERROR 2017-03-07 00:32:07,793 kur.model.executor:647]\u001b[0m Received NaN loss value for model output \"out_char\". Make sure that your inputs are all normalized and that the learning rate is not too high. Sometimes different algorithms/implementations work better than others, so you can try switching optimizers or backend.\u001b[0m\n",
      "Epoch 1/1, loss=nan:   0%|             | 64/13302 [00:00<00:44, 294.78samples/s]\n",
      "\u001b[1;31m[ERROR 2017-03-07 00:32:07,794 kur.model.executor:227]\u001b[0m Exception raised during training.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 224, in train\n",
      "    **kwargs\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 648, in wrapped_train\n",
      "    raise ValueError('Model loss is NaN.')\n",
      "ValueError: Model loss is NaN.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-07 00:32:07,797 kur.model.executor:235]\u001b[0m Saving most recent weights: t3_dlnd/last.w.kur\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 224, in train\n",
      "    **kwargs\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 648, in wrapped_train\n",
      "    raise ValueError('Model loss is NaN.')\n",
      "ValueError: Model loss is NaN.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/bin/kur\", line 11, in <module>\n",
      "    load_entry_point('kur', 'console_scripts', 'kur')()\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 382, in main\n",
      "    sys.exit(args.func(args) or 0)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 62, in train\n",
      "    func(step=args.step)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 371, in func\n",
      "    return trainer.train(**defaults)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/executor.py\", line 246, in train\n",
      "    info={'Reason' : reason}\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/hooks/plot_hook.py\", line 123, in notify\n",
      "    vbatch = numpy.arange(1, len(vloss)+1)\n",
      "TypeError: object of type 'NoneType' has no len()\n"
     ]
    }
   ],
   "source": [
    "!kur -v train char_rrn_demo_dlnd_fluid.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
